{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e68a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e556ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SparkSession.builder.appName(' Home work ').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aead36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = f'./data/fhv_tripdata_2021-01.parquet'\n",
    "test_data = f'./data/fhv_tripdata_2021-02.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9143b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(filename):\n",
    "#     df = sp.read.parquet(filename, inferSchema=True).cache\n",
    "    df = sp.read.parquet(filename, inferSchema=True)\n",
    "#     Getting the number of Record in dataframe\n",
    "    print(\"Number of Row in this Dataframe is : \", record_counter(df))\n",
    "    df = time_conversion(df)\n",
    "    print(\"Average Duration is: \", getAvgDuration(df))\n",
    "    df_betw_1_60 = df_filter(df)\n",
    "    print(\"Number Record droped is : \",record_counter(df) - record_counter(df_betw_1_60) )\n",
    "    df_betw_1_60.show()\n",
    "    print(\"Number of Null/NA for each Row\")\n",
    "    Null_count = count_null(df, 'PUlocationID', 'DOLocationID')\n",
    "    Null_count.show()\n",
    "    print('Percentageof NA in PUlocationID is :')\n",
    "    print((extrac_row_value(Null_count) * 100) / record_counter(df))\n",
    "    df = replace_null(df_betw_1_60, 'PUlocationID', 'DOLocationID')\n",
    "    print(\"Select Needed Column for ML\")\n",
    "    df = selectCol(df)\n",
    "    df = castCol(df)\n",
    "    df = ohe(df)\n",
    "    df = pruneCol(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b12c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_conversion(df):\n",
    "    df = df.withColumn('duration', (unix_timestamp(df.dropOff_datetime) - unix_timestamp(df.pickup_datetime)) / 60)\n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2550b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def castCol(df):\n",
    "    #  Column Casting Process:\n",
    "    feature_columns = df.withColumn(\"PUlocationID\", col(\"PUlocationID\").cast(StringType())) \\\n",
    "                .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(StringType()))\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2575192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectCol(df):\n",
    "    # Feature Extraction\n",
    "    categorical = [\"PULocationID\",\"DOLocationID\"]\n",
    "    numerical = [\"duration\"]\n",
    "    # In future project, tips-amount should be use as label\n",
    "    # Read Article : https://stackoverflow.com/questions/47871874/does-spark-do-one-pass-through-the-data-for-multiple-withcolumn\n",
    "    feature_columns = df.select(categorical + numerical)\n",
    "    \n",
    "    return feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c2d50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "966f56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Implementing One Hot encoding on \"PULocationID\" and \"DOLocationID\" column\n",
    "def ohe(feature_columns):\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    feature_columns = StringIndexer(\n",
    "    inputCol='PUlocationID', \n",
    "    outputCol='Pick_UP', \n",
    "    handleInvalid='keep').fit(feature_columns).transform(feature_columns)\n",
    "    \n",
    "    feature_columns = StringIndexer(\n",
    "    inputCol='DOLocationID', \n",
    "    outputCol='Drop_OFF', \n",
    "    handleInvalid='keep').fit(feature_columns).transform(feature_columns)\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e785572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "def pruneCol(df):\n",
    "    feature_columns = df.drop('PUlocationID', 'DOLocationID')\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b940ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_counter(df):\n",
    "    return df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "190b2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgDuration(df):\n",
    "    return df.agg(avg(col('duration'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8faec268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_filter(df):\n",
    "#     df = df.select(df.where(df.duration.between(1,60)))\n",
    "#     return df.show()\n",
    "#     df.filter(df.where(df.duration.between(1,60)))\n",
    "    return df.filter(df.duration.between(1,61))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "819ebc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/#:~:text=In%20PySpark%20DataFrame%20you%20can,count()%20and%20when().\n",
    "def count_null(df, col1, col2):\n",
    "    df_Columns=[col1,col1]\n",
    "\n",
    "    return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_Columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2eeb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrac_row_value(df):\n",
    "    return df.head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97d2fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_null(df, col1, col2):\n",
    "    return df.na.fill(value=-1,subset=[col1, col2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfcb43e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Row in this Dataframe is :  1154112\n",
      "+----------------+\n",
      "|   avg(duration)|\n",
      "+----------------+\n",
      "|19.1672240937939|\n",
      "+----------------+\n",
      "\n",
      "Average Duration is:  None\n",
      "Number Record droped is :  43014\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|          duration|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|              B00009|2021-01-01 01:27:00|2021-01-01 01:44:00|        null|        null|   null|                B00009|              17.0|\n",
      "|              B00009|2021-01-01 01:50:00|2021-01-01 02:07:00|        null|        null|   null|                B00009|              17.0|\n",
      "|              B00037|2021-01-01 01:13:09|2021-01-01 01:21:26|        null|        72.0|   null|                B00037| 8.283333333333333|\n",
      "|              B00037|2021-01-01 01:38:31|2021-01-01 01:53:44|        null|        61.0|   null|                B00037|15.216666666666667|\n",
      "|              B00037|2021-01-01 01:59:02|2021-01-01 02:08:05|        null|        71.0|   null|                B00037|              9.05|\n",
      "|              B00037|2021-01-01 01:18:12|2021-01-01 01:30:04|        null|        91.0|   null|                B00037|11.866666666666667|\n",
      "|              B00037|2021-01-01 01:36:15|2021-01-01 01:45:08|        null|        39.0|   null|                B00037| 8.883333333333333|\n",
      "|              B00037|2021-01-01 01:55:04|2021-01-01 02:13:02|        null|        37.0|   null|                B00037|17.966666666666665|\n",
      "|              B00037|2021-01-01 01:48:40|2021-01-01 02:12:02|        null|        39.0|   null|                B00037|23.366666666666667|\n",
      "|              B00037|2021-01-01 01:27:23|2021-01-01 01:29:25|        null|        72.0|   null|                B00037| 2.033333333333333|\n",
      "|              B00037|2021-01-01 01:39:07|2021-01-01 01:43:26|        null|        72.0|   null|                B00037| 4.316666666666666|\n",
      "|              B00037|2021-01-01 01:55:57|2021-01-01 02:10:08|        null|        89.0|   null|                B00037|14.183333333333334|\n",
      "|              B00095|2021-01-01 01:17:54|2021-01-01 01:20:57|        null|       177.0|   null|                B00095|              3.05|\n",
      "|              B00095|2021-01-01 01:31:59|2021-01-01 01:42:41|        null|       225.0|   null|                B00095|              10.7|\n",
      "|              B00095|2021-01-01 01:46:23|2021-01-01 01:57:33|        null|        63.0|   null|                B00095|11.166666666666666|\n",
      "|              B00111|2021-01-01 01:40:00|2021-01-01 02:41:00|        null|        null|   null|                B03234|              61.0|\n",
      "|              B00112|2021-01-01 01:21:10|2021-01-01 01:42:42|        null|        67.0|   null|                B00112|21.533333333333335|\n",
      "|              B00112|2021-01-01 01:53:10|2021-01-01 02:03:24|        null|        22.0|   null|                B00112|10.233333333333333|\n",
      "|              B00112|2021-01-01 01:42:38|2021-01-01 02:15:46|        null|        61.0|   null|                B00112| 33.13333333333333|\n",
      "|              B00112|2021-01-01 01:54:48|2021-01-01 02:18:21|        null|        14.0|   null|                B00112|             23.55|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of Null/NA for each Row\n",
      "+------------+------------+\n",
      "|PUlocationID|PUlocationID|\n",
      "+------------+------------+\n",
      "|      958267|      958267|\n",
      "+------------+------------+\n",
      "\n",
      "Percentageof NA in PUlocationID is :\n",
      "83.03067639882438\n",
      "Select Needed Column for ML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------+\n",
      "|          duration|Pick_UP|Drop_OFF|\n",
      "+------------------+-------+--------+\n",
      "|              17.0|    0.0|     0.0|\n",
      "|              17.0|    0.0|     0.0|\n",
      "| 8.283333333333333|    0.0|    30.0|\n",
      "|15.216666666666667|    0.0|     5.0|\n",
      "|              9.05|    0.0|    67.0|\n",
      "|11.866666666666667|    0.0|    76.0|\n",
      "| 8.883333333333333|    0.0|    42.0|\n",
      "|17.966666666666665|    0.0|    24.0|\n",
      "|23.366666666666667|    0.0|    42.0|\n",
      "| 2.033333333333333|    0.0|    30.0|\n",
      "| 4.316666666666666|    0.0|    30.0|\n",
      "|14.183333333333334|    0.0|    55.0|\n",
      "|              3.05|    0.0|    46.0|\n",
      "|              10.7|    0.0|    40.0|\n",
      "|11.166666666666666|    0.0|    34.0|\n",
      "|              61.0|    0.0|     0.0|\n",
      "|21.533333333333335|    0.0|   109.0|\n",
      "|10.233333333333333|    0.0|    64.0|\n",
      "| 33.13333333333333|    0.0|     5.0|\n",
      "|             23.55|    0.0|    45.0|\n",
      "+------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Test\n",
    "train_data = get_dataframe(train_data)\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4c87cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Row in this Dataframe is :  1037692\n",
      "+-----------------+\n",
      "|    avg(duration)|\n",
      "+-----------------+\n",
      "|20.70698622520125|\n",
      "+-----------------+\n",
      "\n",
      "Average Duration is:  None\n",
      "Number Record droped is :  46100\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|          duration|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|     B00021         |2021-02-01 01:55:40|2021-02-01 02:06:20|       173.0|        82.0|   null|       B00021         |10.666666666666666|\n",
      "|     B00021         |2021-02-01 01:14:03|2021-02-01 01:28:37|       173.0|        56.0|   null|       B00021         |14.566666666666666|\n",
      "|     B00021         |2021-02-01 01:27:48|2021-02-01 01:35:45|        82.0|       129.0|   null|       B00021         |              7.95|\n",
      "|              B00037|2021-02-01 01:12:50|2021-02-01 01:26:38|        null|       225.0|   null|                B00037|              13.8|\n",
      "|              B00037|2021-02-01 01:00:37|2021-02-01 01:09:35|        null|        61.0|   null|                B00037| 8.966666666666667|\n",
      "|              B00112|2021-02-01 01:30:25|2021-02-01 01:57:23|        null|        26.0|   null|                B00112|26.966666666666665|\n",
      "|              B00149|2021-02-01 01:43:16|2021-02-01 02:03:16|        null|        72.0|   null|                B00149|              20.0|\n",
      "|              B00225|2021-02-01 01:23:27|2021-02-01 01:55:46|        null|       169.0|   null|                B00225| 32.31666666666667|\n",
      "|              B00225|2021-02-01 01:10:38|2021-02-01 01:50:15|        null|       161.0|   null|                B02872| 39.61666666666667|\n",
      "|              B00254|2021-02-01 01:05:46|2021-02-01 01:40:41|        13.0|       182.0|   null|                B00254|34.916666666666664|\n",
      "|              B00254|2021-02-01 01:14:25|2021-02-01 01:24:56|       152.0|       244.0|   null|                B02872|10.516666666666667|\n",
      "|              B00256|2021-02-01 01:39:11|2021-02-01 02:18:44|        null|        null|   null|                B00256|             39.55|\n",
      "|              B00256|2021-02-01 01:33:24|2021-02-01 02:23:44|        null|        null|   null|                B00256|50.333333333333336|\n",
      "|              B00256|2021-02-01 01:05:19|2021-02-01 01:24:40|        null|        null|   null|                B00256|             19.35|\n",
      "|              B00271|2021-02-01 01:04:07|2021-02-01 02:03:03|        null|       265.0|   null|                B00271| 58.93333333333333|\n",
      "|              B00271|2021-02-01 01:07:13|2021-02-01 01:08:49|        null|       237.0|   null|                B00271|               1.6|\n",
      "|              B00310|2021-02-01 01:11:21|2021-02-01 01:15:44|        null|       248.0|   null|                B00310| 4.383333333333334|\n",
      "|              B00310|2021-02-01 01:27:03|2021-02-01 01:30:20|        null|       248.0|   null|                B00310| 3.283333333333333|\n",
      "|              B00310|2021-02-01 01:36:39|2021-02-01 01:59:25|        null|       159.0|   null|                B00310|22.766666666666666|\n",
      "|              B00310|2021-02-01 01:37:16|2021-02-01 02:06:22|        null|       213.0|   null|                B02788|              29.1|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of Null/NA for each Row\n",
      "+------------+------------+\n",
      "|PUlocationID|PUlocationID|\n",
      "+------------+------------+\n",
      "|      884691|      884691|\n",
      "+------------+------------+\n",
      "\n",
      "Percentageof NA in PUlocationID is :\n",
      "85.25564425667731\n",
      "Select Needed Column for ML\n"
     ]
    }
   ],
   "source": [
    "test_data = get_dataframe(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03aa05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble all the features with VectorAssembler\n",
    "def feature_assembler(df):\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    x_features = ['Pick_UP',\n",
    "                    'Drop_OFF'                   ]\n",
    "    feature_assembler = VectorAssembler(\n",
    "    inputCols=x_features, \n",
    "    outputCol='features')\n",
    "    transformed_data = feature_assembler.transform(df)\n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c68b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data = feature_assembler(train_data)\n",
    "transformed_test_data = feature_assembler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a05e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 22:19:29 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/05/22 22:19:29 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "[Stage 95:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.03965500955265982,0.006032146879372192]\n",
      "Intercept: 15.638580165296206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  Apply Model Function\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='duration', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(transformed_train_data)\n",
    "\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6c38ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.532699\n",
      "r2: 0.018638\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2323ca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------+\n",
      "|        prediction|          duration|    features|\n",
      "+------------------+------------------+------------+\n",
      "|16.456594743122874|10.666666666666666| [15.0,37.0]|\n",
      "|16.679784177659645|14.566666666666666| [15.0,74.0]|\n",
      "| 16.34969400822364|              7.95| [12.0,39.0]|\n",
      "|15.910026774867955|              13.8|  [0.0,45.0]|\n",
      "|15.668740899693066| 8.966666666666667|   [0.0,5.0]|\n",
      "|  15.7049337809693|26.966666666666665|  [0.0,11.0]|\n",
      "|15.831608865436117|              20.0|  [0.0,32.0]|\n",
      "|15.686837340331184| 32.31666666666667|   [0.0,8.0]|\n",
      "|16.507209315925802| 39.61666666666667| [0.0,144.0]|\n",
      "| 24.14352813904201|34.916666666666664|[202.0,82.0]|\n",
      "|19.408376091401834|10.516666666666667|  [94.0,7.0]|\n",
      "|15.638580165296206|             39.55|   (2,[],[])|\n",
      "|15.638580165296206|50.333333333333336|   (2,[],[])|\n",
      "|15.638580165296206|             19.35|   (2,[],[])|\n",
      "| 15.65064445905495| 58.93333333333333|   [0.0,2.0]|\n",
      "|16.525305756563917|               1.6| [0.0,147.0]|\n",
      "|15.958283949902933| 4.383333333333334|  [0.0,53.0]|\n",
      "|15.958283949902933| 3.283333333333333|  [0.0,53.0]|\n",
      "|15.807480277918627|22.766666666666666|  [0.0,28.0]|\n",
      "|15.741126662245533|              29.1|  [0.0,17.0]|\n",
      "+------------------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 104:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 12.0051\n",
      "<bound method DataFrame.summary of DataFrame[duration: double, Pick_UP: double, Drop_OFF: double, features: vector, prediction: double]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Model validation\n",
    "# Test Prediction\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions = lr_model.transform(transformed_test_data)\n",
    "predictions.select(\"prediction\",\"duration\",\"features\").show()\n",
    "lr_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"duration\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = lr_evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e0be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27001c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python399jvsc74a57bd06e5ff1f42187c785f96509ab8359de136089755a3bbf2a06995080c4b5fa1cab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
