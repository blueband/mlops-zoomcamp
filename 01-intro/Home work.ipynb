{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e68a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e556ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SparkSession.builder.appName(' Home work ').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aead36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = f'./data/fhv_tripdata_2021-01.parquet'\n",
    "test_data = f'./data/fhv_tripdata_2021-02.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9143b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(filename):\n",
    "#     df = sp.read.parquet(filename, inferSchema=True).cache\n",
    "    df = sp.read.parquet(filename, inferSchema=True)\n",
    "#     Getting the number of Record in dataframe\n",
    "    print(\"Number of Row in this Dataframe is : \", record_counter(df))\n",
    "    df = time_conversion(df)\n",
    "    print(\"Average Duration is: \", getAvgDuration(df))\n",
    "    df_betw_1_60 = df_filter(df)\n",
    "    print(\"Number Record droped is : \",record_counter(df) - record_counter(df_betw_1_60) )\n",
    "    df_betw_1_60.show()\n",
    "    print(\"Number of Null/NA for each Row\")\n",
    "    Null_count = count_null(df, 'PUlocationID', 'DOLocationID')\n",
    "    Null_count.show()\n",
    "    print('Percentageof NA in PUlocationID is :')\n",
    "    print((extrac_row_value(Null_count) * 100) / record_counter(df))\n",
    "    df = replace_null(df_betw_1_60, 'PUlocationID', 'DOLocationID')\n",
    "    print(\"Select Needed Column for ML\")\n",
    "    df = selectCol(df)\n",
    "    df = castCol(df)\n",
    "    df = ohe(df)\n",
    "    df = pruneCol(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b12c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_conversion(df):\n",
    "    df = df.withColumn('duration', (unix_timestamp(df.dropOff_datetime) - unix_timestamp(df.pickup_datetime)) / 60)\n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2550b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def castCol(df):\n",
    "    #  Column Casting Process:\n",
    "    feature_columns = df.withColumn(\"PUlocationID\", col(\"PUlocationID\").cast(StringType())) \\\n",
    "                .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(StringType()))\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2575192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectCol(df):\n",
    "    # Feature Extraction\n",
    "    categorical = [\"PULocationID\",\"DOLocationID\"]\n",
    "    numerical = [\"duration\"]\n",
    "    # In future project, tips-amount should be use as label\n",
    "    # Read Article : https://stackoverflow.com/questions/47871874/does-spark-do-one-pass-through-the-data-for-multiple-withcolumn\n",
    "    feature_columns = df.select(categorical + numerical)\n",
    "    \n",
    "    return feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c2d50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "966f56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Implementing One Hot encoding on \"PULocationID\" and \"DOLocationID\" column\n",
    "def ohe(feature_columns):\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    feature_columns = StringIndexer(\n",
    "    inputCol='PUlocationID', \n",
    "    outputCol='Pick_UP', \n",
    "    handleInvalid='keep').fit(feature_columns).transform(feature_columns)\n",
    "    \n",
    "    feature_columns = StringIndexer(\n",
    "    inputCol='DOLocationID', \n",
    "    outputCol='Drop_OFF', \n",
    "    handleInvalid='keep').fit(feature_columns).transform(feature_columns)\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e785572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "def pruneCol(df):\n",
    "    feature_columns = df.drop('PUlocationID', 'DOLocationID')\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b940ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_counter(df):\n",
    "    return df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "190b2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgDuration(df):\n",
    "    return df.agg(avg(col('duration'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8faec268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_filter(df):\n",
    "#     df = df.select(df.where(df.duration.between(1,60)))\n",
    "#     return df.show()\n",
    "#     df.filter(df.where(df.duration.between(1,60)))\n",
    "    return df.filter(df.duration.between(1,61))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "819ebc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/#:~:text=In%20PySpark%20DataFrame%20you%20can,count()%20and%20when().\n",
    "def count_null(df, col1, col2):\n",
    "    df_Columns=[col1,col1]\n",
    "\n",
    "    return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_Columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2eeb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrac_row_value(df):\n",
    "    return df.head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97d2fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_null(df, col1, col2):\n",
    "    return df.na.fill(value=-1,subset=[col1, col2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfcb43e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Row in this Dataframe is :  1154112\n",
      "+----------------+\n",
      "|   avg(duration)|\n",
      "+----------------+\n",
      "|19.1672240937939|\n",
      "+----------------+\n",
      "\n",
      "Average Duration is:  None\n",
      "Number Record droped is :  43014\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|          duration|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|              B00009|2021-01-01 01:27:00|2021-01-01 01:44:00|        null|        null|   null|                B00009|              17.0|\n",
      "|              B00009|2021-01-01 01:50:00|2021-01-01 02:07:00|        null|        null|   null|                B00009|              17.0|\n",
      "|              B00037|2021-01-01 01:13:09|2021-01-01 01:21:26|        null|        72.0|   null|                B00037| 8.283333333333333|\n",
      "|              B00037|2021-01-01 01:38:31|2021-01-01 01:53:44|        null|        61.0|   null|                B00037|15.216666666666667|\n",
      "|              B00037|2021-01-01 01:59:02|2021-01-01 02:08:05|        null|        71.0|   null|                B00037|              9.05|\n",
      "|              B00037|2021-01-01 01:18:12|2021-01-01 01:30:04|        null|        91.0|   null|                B00037|11.866666666666667|\n",
      "|              B00037|2021-01-01 01:36:15|2021-01-01 01:45:08|        null|        39.0|   null|                B00037| 8.883333333333333|\n",
      "|              B00037|2021-01-01 01:55:04|2021-01-01 02:13:02|        null|        37.0|   null|                B00037|17.966666666666665|\n",
      "|              B00037|2021-01-01 01:48:40|2021-01-01 02:12:02|        null|        39.0|   null|                B00037|23.366666666666667|\n",
      "|              B00037|2021-01-01 01:27:23|2021-01-01 01:29:25|        null|        72.0|   null|                B00037| 2.033333333333333|\n",
      "|              B00037|2021-01-01 01:39:07|2021-01-01 01:43:26|        null|        72.0|   null|                B00037| 4.316666666666666|\n",
      "|              B00037|2021-01-01 01:55:57|2021-01-01 02:10:08|        null|        89.0|   null|                B00037|14.183333333333334|\n",
      "|              B00095|2021-01-01 01:17:54|2021-01-01 01:20:57|        null|       177.0|   null|                B00095|              3.05|\n",
      "|              B00095|2021-01-01 01:31:59|2021-01-01 01:42:41|        null|       225.0|   null|                B00095|              10.7|\n",
      "|              B00095|2021-01-01 01:46:23|2021-01-01 01:57:33|        null|        63.0|   null|                B00095|11.166666666666666|\n",
      "|              B00111|2021-01-01 01:40:00|2021-01-01 02:41:00|        null|        null|   null|                B03234|              61.0|\n",
      "|              B00112|2021-01-01 01:21:10|2021-01-01 01:42:42|        null|        67.0|   null|                B00112|21.533333333333335|\n",
      "|              B00112|2021-01-01 01:53:10|2021-01-01 02:03:24|        null|        22.0|   null|                B00112|10.233333333333333|\n",
      "|              B00112|2021-01-01 01:42:38|2021-01-01 02:15:46|        null|        61.0|   null|                B00112| 33.13333333333333|\n",
      "|              B00112|2021-01-01 01:54:48|2021-01-01 02:18:21|        null|        14.0|   null|                B00112|             23.55|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of Null/NA for each Row\n",
      "+------------+------------+\n",
      "|PUlocationID|PUlocationID|\n",
      "+------------+------------+\n",
      "|      958267|      958267|\n",
      "+------------+------------+\n",
      "\n",
      "Percentageof NA in PUlocationID is :\n",
      "83.03067639882438\n",
      "Select Needed Column for ML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------+\n",
      "|          duration|Pick_UP|Drop_OFF|\n",
      "+------------------+-------+--------+\n",
      "|              17.0|    0.0|     0.0|\n",
      "|              17.0|    0.0|     0.0|\n",
      "| 8.283333333333333|    0.0|    30.0|\n",
      "|15.216666666666667|    0.0|     5.0|\n",
      "|              9.05|    0.0|    67.0|\n",
      "|11.866666666666667|    0.0|    76.0|\n",
      "| 8.883333333333333|    0.0|    42.0|\n",
      "|17.966666666666665|    0.0|    24.0|\n",
      "|23.366666666666667|    0.0|    42.0|\n",
      "| 2.033333333333333|    0.0|    30.0|\n",
      "| 4.316666666666666|    0.0|    30.0|\n",
      "|14.183333333333334|    0.0|    55.0|\n",
      "|              3.05|    0.0|    46.0|\n",
      "|              10.7|    0.0|    40.0|\n",
      "|11.166666666666666|    0.0|    34.0|\n",
      "|              61.0|    0.0|     0.0|\n",
      "|21.533333333333335|    0.0|   109.0|\n",
      "|10.233333333333333|    0.0|    64.0|\n",
      "| 33.13333333333333|    0.0|     5.0|\n",
      "|             23.55|    0.0|    45.0|\n",
      "+------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Test\n",
    "train_data = get_dataframe(train_data)\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03aa05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble all the features with VectorAssembler\n",
    "def feature_assembler(df):\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    x_features = ['Pick_UP',\n",
    "                    'Drop_OFF'                   ]\n",
    "    feature_assembler = VectorAssembler(\n",
    "    inputCols=x_features, \n",
    "    outputCol='features')\n",
    "    transformed_data = feature_assembler.transform(df)\n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c68b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data = feature_assembler(train_data)\n",
    "transformed_test_data = feature_assembler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a05e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 21:53:41 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/05/22 21:53:41 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "[Stage 65:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.03965500955265982,0.006032146879372192]\n",
      "Intercept: 15.638580165296206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  Apply Model Function\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='duration', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(transformed_train_data)\n",
    "\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6c38ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.532699\n",
      "r2: 0.018638\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation\n",
    "# Test Prediction\n",
    "predictions = lr_model.transform(transformed_test_data)\n",
    "predictions.select(\"prediction\",\"duration\",\"features\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python399jvsc74a57bd06e5ff1f42187c785f96509ab8359de136089755a3bbf2a06995080c4b5fa1cab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
