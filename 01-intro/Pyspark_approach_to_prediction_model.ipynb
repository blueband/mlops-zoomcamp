{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2138bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe1ae0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 22:01:07 WARN Utils: Your hostname, blueberry resolves to a loopback address: 127.0.1.1; using 192.168.43.179 instead (on interface wlp2s0)\n",
      "22/05/22 22:01:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/blueberry/.local/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/22 22:01:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/22 22:01:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/22 22:01:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "sp = SparkSession.builder.appName(' Trip Machile Learning ').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f948c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = f'./data/green_tripdata_2021-01.parquet'\n",
    "test_data = f'./data/green_tripdata_2021-02.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ac821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(filename):\n",
    "#     df = sp.read.parquet(filename, inferSchema=True).cache\n",
    "    df = sp.read.parquet(filename, inferSchema=True)\n",
    "    df = time_conversion(df)\n",
    "    df = selectCol(df)\n",
    "    df = castCol(df)\n",
    "    df = ohe(df)\n",
    "    df = pruneCol(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53df9229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_conversion(df):\n",
    "    df = df.withColumn('duration', (unix_timestamp(df.lpep_dropoff_datetime) - unix_timestamp(df.lpep_pickup_datetime)) / 60)\n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7065036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def castCol(df):\n",
    "    #  Column Casting Process:\n",
    "    feature_columns = df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(StringType())) \\\n",
    "                .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(StringType()))\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09571ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectCol(df):\n",
    "    # Feature Extraction\n",
    "    categorical = [\"PULocationID\",\"DOLocationID\"]\n",
    "    numerical = [\"trip_distance\", \"duration\"]\n",
    "    # In future project, tips-amount should be use as label\n",
    "    # Read Article : https://stackoverflow.com/questions/47871874/does-spark-do-one-pass-through-the-data-for-multiple-withcolumn\n",
    "    feature_columns = df.select(categorical + numerical)\n",
    "    \n",
    "    return feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658e4d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62731a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Implementing One Hot encoding on \"PULocationID\" and \"DOLocationID\" column\n",
    "def ohe(feature_columns):\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    feature_columns = StringIndexer(\n",
    "    inputCol='PULocationID', \n",
    "    outputCol='Pick_UP', \n",
    "    handleInvalid='keep').fit(feature_columns).transform(feature_columns)\n",
    "    \n",
    "    feature_columns = StringIndexer(\n",
    "    inputCol='DOLocationID', \n",
    "    outputCol='Drop_OFF', \n",
    "    handleInvalid='keep').fit(feature_columns).transform(feature_columns)\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "820d0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "def pruneCol(df):\n",
    "    feature_columns = df.drop('PULocationID', 'DOLocationID')\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dffabb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data = get_dataframe(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d72bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_dataframe(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16fb8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble all the features with VectorAssembler\n",
    "def feature_assembler(df):\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    x_features = ['Pick_UP',\n",
    "                    'Drop_OFF',\n",
    "                    'trip_distance'\n",
    "                   ]\n",
    "    feature_assembler = VectorAssembler(\n",
    "    inputCols=x_features, \n",
    "    outputCol='features')\n",
    "    transformed_data = feature_assembler.transform(df)\n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8ec4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data = feature_assembler(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1eb05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_data = feature_assembler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fa540dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/22 22:01:40 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/05/22 22:01:40 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.031216386369037438,0.046011884287015775,0.0]\n",
      "Intercept: 16.228208486650043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  Apply Model Function\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='duration', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(transformed_train_data)\n",
    "\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8267184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 59.234099\n",
      "r2: 0.003506\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e0e0da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-----------------+\n",
      "|        prediction|           duration|         features|\n",
      "+------------------+-------------------+-----------------+\n",
      "|  20.0389009466867| 17.916666666666668| [13.0,74.0,3.66]|\n",
      "|17.409554997074224|                6.5|   [29.0,6.0,1.1]|\n",
      "| 19.61812544285098|              15.25| [29.0,54.0,4.93]|\n",
      "|21.090505740035486| 18.233333333333334|  [29.0,86.0,6.7]|\n",
      "|16.305436757306097|  8.966666666666667|   [1.0,1.0,1.89]|\n",
      "|24.355724133880525|               7.85| [26.0,159.0,3.3]|\n",
      "| 20.88191036177096|                9.7| [12.0,93.0,2.51]|\n",
      "| 18.38247311235413| 11.283333333333333| [13.0,38.0,1.68]|\n",
      "| 23.61903303592752|  8.733333333333333|[110.0,86.0,1.44]|\n",
      "|21.737755917999618| 1.7166666666666666|  [63.0,77.0,0.0]|\n",
      "|22.574430754071805|               11.8| [22.0,123.0,1.9]|\n",
      "| 21.99578094473757|  9.766666666666667|  [58.0,86.0,1.9]|\n",
      "|16.489484294454158| 10.133333333333333|   [1.0,5.0,1.73]|\n",
      "|17.639614418509304|  4.133333333333334| [29.0,11.0,0.94]|\n",
      "| 18.57643997582093|                7.9|  [3.0,49.0,3.24]|\n",
      "|18.300368670098834|                9.0|  [3.0,43.0,1.67]|\n",
      "| 20.77817489322486|  6.216666666666667|  [47.0,67.0,1.2]|\n",
      "| 20.48893348011787|0.11666666666666667| [51.0,58.0,0.19]|\n",
      "| 17.07771946386663|0.06666666666666667|  [11.0,11.0,0.0]|\n",
      "|19.553900296459574|  5.716666666666667| [52.0,37.0,0.93]|\n",
      "+------------------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'rootMeanSquaredError'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m predictions\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      4\u001b[0m predictionsSummary \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39msummary\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[43mpredictionsSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrootMeanSquaredError\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr2: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m predictionsSummary\u001b[38;5;241m.\u001b[39mr2)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'rootMeanSquaredError'"
     ]
    }
   ],
   "source": [
    "# Test Prediction\n",
    "predictions = lr_model.transform(transformed_test_data)\n",
    "predictions.select(\"prediction\",\"duration\",\"features\").show()\n",
    "predictionsSummary = predictions.summary\n",
    "print(\"RMSE: %f\" % predictionsSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % predictionsSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c64417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree regression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'duration', maxBins=260)\n",
    "dt_model = dt.fit(transformed_train_data)\n",
    "dt_predictions = dt_model.transform(transformed_test_data)\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"duration\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "dt_predictions.select(\"prediction\",\"duration\",\"features\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0357842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 352:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 105.668\n",
      "+------------------+-------------------+-----------------+\n",
      "|        prediction|           duration|         features|\n",
      "+------------------+-------------------+-----------------+\n",
      "|25.929506554117005| 17.916666666666668| [13.0,74.0,3.66]|\n",
      "| 5.068813123953247|                6.5|   [29.0,6.0,1.1]|\n",
      "|16.793573093296956|              15.25| [29.0,54.0,4.93]|\n",
      "|14.539506983903118| 18.233333333333334|  [29.0,86.0,6.7]|\n",
      "|12.941325549113841|  8.966666666666667|   [1.0,1.0,1.89]|\n",
      "|23.884582022398117|               7.85| [26.0,159.0,3.3]|\n",
      "|  18.9439868171003|                9.7| [12.0,93.0,2.51]|\n",
      "|10.682351751041299| 11.283333333333333| [13.0,38.0,1.68]|\n",
      "|3.4740774108408927|  8.733333333333333|[110.0,86.0,1.44]|\n",
      "| 13.96829662139847| 1.7166666666666666|  [63.0,77.0,0.0]|\n",
      "| 10.40512369640853|               11.8| [22.0,123.0,1.9]|\n",
      "| 0.678928633980451|  9.766666666666667|  [58.0,86.0,1.9]|\n",
      "|12.613555119252418| 10.133333333333333|   [1.0,5.0,1.73]|\n",
      "| 9.430674966983894|  4.133333333333334| [29.0,11.0,0.94]|\n",
      "|15.323034819775726|                7.9|  [3.0,49.0,3.24]|\n",
      "| 12.64330412278241|                9.0|  [3.0,43.0,1.67]|\n",
      "| 6.794361787140202|  6.216666666666667|  [47.0,67.0,1.2]|\n",
      "|  45.2644900942769|0.11666666666666667| [51.0,58.0,0.19]|\n",
      "|20.786866857363332|0.06666666666666667|  [11.0,11.0,0.0]|\n",
      "|24.731159437888415|  5.716666666666667| [52.0,37.0,0.93]|\n",
      "+------------------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Gradient-boosted tree regression\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'duration', maxIter=10, maxBins=260)\n",
    "gbt_model = gbt.fit(transformed_train_data)\n",
    "gbt_predictions = gbt_model.transform(transformed_test_data)\n",
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"duration\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48dc2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
